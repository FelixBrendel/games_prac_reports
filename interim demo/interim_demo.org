* Progress in Gameplay
# mention layers of developement?
For the interim demo we made the decision to focus on gameplay mechanics. As a
lot of work went into building our own engine from scratch, we left the visuals
at a basic level for the time being. In the following we elaborate on our
progress so far. \\

In terms of gameplay we have been able finish up layer 1 and 2 mentioned in our
proposal. This means that the player is able to move the cube on the field by
using the W,A,S,D buttons. As intended the cube slides on slippery tiles without
changing its orientation until it hits an obstacle or a wall. The main task then
is to navigate the cube through the level in order to get to the finish tile.
Inputs can further be buffered while the cube is moving. This makes the cube
instantly move into that direction when it arrives at a location.\\

# screenshot of lvl1
#+caption: Sample Level 1 including the movable cube, blue slippery tiles, 
#+caption: the red finish tile and brown obstacle tiles
#+name: Fig:Lvl1
#+attr_latex: :options [htbp] :width 0.4\textwidth
#+begin_figure 
#+begin_center
#+name: Fig:Level1
 [[../images/lvl1.png]] 
#+end_center
#+end_figure

When moving onto a dry tile, the cube will flip into that direction. In order to
finish these levels the cube would have to reach the finish tile in one of three
specific orientations. For that we color-coded the sides as red, green and blue
with one color represented on two opposites sides of the cube. Thanks to the
angle of the camera the player can always observe all different sides of the
cube and plan ahead. Furthermore depending on the level the finish tile can be
slippery or dry adding flexibility to level designs.

# screenshot of lvl2
#+caption: Sample Level 2 including the green dry tiles and red finish with green
#+caption: circle that requires the green side of the cube to be on top/bottom
#+name: Fig:Lvl2
#+attr_latex: :options [htbp] :width 0.4\textwidth
#+begin_figure
#+begin_center
#+name: Fig:Level2
 [[../images/lvl2.png]] 
#+end_center
#+end_figure

* Engine Work
As the development of our own game engine makes up the major part of our project
we will first go over our current progress on the specific components that we
have implemented so far and how they relate to the layers of development. The
engine was programmed in C++ while using the Vulkan Graphics API for the render
pipeline.
** Vulkan Initialization
The Vulkan Graphics API is a fairly young graphics API developed by the Khronos
Group that was developed with the intent of leaving a higher amount of control
to the developer. In turn this low level access means that the initialization
and creation of the render pipeline is more extensive when compared to
high-level level API's like OpenGL.
Before we could start with the initialization we had to install a graphics
driver that supports the Vulkan API. As Vulkan can be used on different types of
devices and operating systems we had to some additional basic setup to use
Vulkan in our engine beyond finding a driver that supports Vulkan and runs on
our hardware.
*** Validation Layers
One one way for the Vulkan developers to increase the performance of the
graphics API was to decrease the amount of error checking done by the driver.
The Vulkan driver is mostly a direct abstraction except for cross platform
functionalities of the hardware. As no type of error checking would mean that is
very difficult to use this API the Khronos Group decided to allow the use of
additional debugging layers called "Validation Layers" that work right in
between the user's application calls and the API. These Validation layers are
part of the SDK and allow validating parameters, validating texture and render
target formats, tracking Vulkan objects and their lifetime and much more. The
validtation capabilities that we use are all packaged into a single layer called
VK_LAYERLUNARG_standard_validation. More layers can be addedd and activated
at the same time.
*** Loading the Vulkan Library
When you want your application to use the Vulkan API you have to use the vulkan
loader which then connects the Vulkan API calls to the appropriate graphics
driver. The Vulkan Loader is a dynamic library and is installed as part of the
SDK provided by lunarg. To use the API functions in our enginge we just had to
use the LoadLibrary() function on the .dll file. After the library is loaded we
can then use a Vulkan-specific function that provides the addresses to all
Vulkan API functions. These functions can be divided into three levels: global,
instance and device. Device level functions perform all the operations that are
needed for 3D rendering such as drawing, the creation of shader modules,
buffers, images, samplers and data copying. Instance level functions create a
logical device but before we can do that we first have to create an instance. To
create a Vulkan instance we have to use a global level function.
*** Vulkan Instances & Extensions
A Vulkan instance is an object that gathers the state of the an application and
allow us to create a logical device. Information that is part of the instance is
the application name, name of the engine used to create an application and the
enabled instance level extension and layers. Extensions are needed to allow
extra functionality for the instance that they are enabled on but have to be
hardware compatible. Example of extensions that we use are swapchain related
extensions and to create an OS level window for our application. Extension can
be enabled on the instance or the device level.
*** Creating a Vulkan instance
After preparing the application info, the number of extensions and the list of
extensions we can then store these values in the InstanceCreateInfo struct. The
Vulkan function vkCreateInstance then just receives the adress to this struct in
which all the needed information for creation is stored. The other arguments for
the instance creation are the address of the instance handle we want to create
and an argement for allocation callback functions which we have not used so far.
The function then returns a enum of type vkResult which provides feedback
whether the function call was able to be resolved correctly. (CODE)
*** Vulkan structs and CreateInfo
To create an instance we have to provde the aforementioned application info,
extensions and validation layers we want to enable to the InstanceCreateInfo
struct. The application info is also saved as a struct which means that we use
nested structs to incorporate all the info for the instance. This also
represents the general philosophy used in Vulkan. All information that
determines the functionality of your use of the Vulkan API is stored in the
according CreateInfo struct which is then used to create the corresponding
object. For better identification and to improve readability the first attribute
of the Vulkan structs is always reserved to identify which information the
struct stores.
*** Physical Device
After creating a Vulkan instance we can then use instance level functions to
gather information about the physical devices (the GPU) available on the
hardware. A Vulkan application can be run on many different devices which can
each incorporate wildly different hardware that have different perfomance and
different capabilities. In order of making sure to chose the correct hardware
(onboard GPU and stand alone GPU card) and to check if the available hardware is
capable of running our application we first gather information about it and
confirm that all features are supported. To do this Vulkan provide three
functions: EnumeratePhysicalDevices() which stores a representation of all
available physical devices, vkGetPhysicalDeviceFeatures which stores the
available features , and vkGetPhysicalDeviceProperties which stores general
information about the physical device. We can then use this information to
choose a suitable phyiscal and then create a logical device from it.
*** Logical Device
Logical devices perform most of the work in Vulkan: we can create resources,
manage memory, record command buffers, submit commands for processing ect.
Bottom line is that they include all the functionality we need to create a
render pipeline. A logical device represents a physical device (GPU) but it is
including all the features and extension we have previously activated and the
information about the queues that can be requested from it. To create a logical
device from a physical device we also need to determine which queue families are
supported by that physical device and which queue families we want to use.
*** Device Queues
The control of the hardware in Vulkan is implemented through queues. Commands
are recorded and then submitted to a queue where they are processed by the
hardware in the order they are submitted. There are different types of queues
which are processed independently and there can be multiple queues of one type
but these processed according to a priority. Different types of Queues are
needed as not all operations are allowed to be performed on all queues. Most
noteable examples of queue operations are graphics, compute and transfer. These
Queues are not created explicitly by the application but have to be requested
from the physical device and are then created with the creation of the logical
device, which means that the supported queue's may differ from hardware to
hardware. To check the properties and available queue families of our physical
device we used the
"CheckAvailableQueueFamiliesAndTheirProperties(physicalDevice, target)" (CODE)
function. This commands stores the queue families, a bit-vector that describes
the operations that are usable by each family and the number of queues each
family supports in the target vector. For the development of our game and engine
we all used NVIDIA GPU's which have one queue family and 16 different avilable
queues that all support graphics, compute, transfer and sparse binding
operations.
** Creation of the render pipeline
Just as the Initialization is more elaborate compared to other high level
Graphics API's, Vulkans render pipeline is also compromised of many elements
that we will further explain in this chapter.
*** Window System Integration extension
Due to the intended portable, cross-platform nature of Vulkan it is not possible
to directly display an image in the applications window as there is no universal
standard for presenting images for different operating systems. Each operating
system for which Vulkan is available has its own Windowing System Integration
extension (WSI) which allows to integrate Vulkan with the OS's specific
windowing system. To enable the use of a Vulkan represenation of an application
window we had to first add the following instance level extensions during
instance creation:
- VK_KHR_SURFACE_EXTENSION_NAME
- VK_KHR_WIN32_SURFACE_EXTENSION_NAME
After adding the instance level extensions we also have to add these two device level extensions:
- VK_KHR_win32_surface which allows our application to manage and create presentation surfaces
- VK_KHR_surface which allows us to destroy presentation surfaces (close the window)
For our engine we used the GLFW extension which also handles our user inputs.
*** Presentation surface
The presenation surface represents the application's window. From the
presentation window we can gather important information like the dimensions of
the window, the supported color formats, presentation modes and if a physical
device is able to display an image in the given window. This means that it is
possible to create presentation surface first to determine which physical device
is best suited to display images in that window (browser applications vs 3D
simulations). To create a presentation surface we first have to create a window
from which the presentation surface collects the CreateWindow() and the
GetModuleHandle(nullptr) values. These are then consolidated in a CreateInfo
struct and passed to the vkCreateWin32SurfaceKHR method.
*** Presentation mode
The presentation mode determines for example whether we use a backbuffer which
will be important for the swapchain creation. The four Vulkan supported
presentation modes are IMMEDIATE, FIFO RELAXED, RELAXED and MAILBOX. Depending
on the hardware used some modes are not usable which was the case for some of
our members.
*** Creating a swapchain
A swapchain is a number of virtual framebuffers managed by the presentaiton
engine that are used by a GPU to stabilize the frame rate. A swapchain consists
of atleast two buffers, one screenbuffer that is used to represent the current
image on the screen and one backbuffer into which the GPU renders the next frame
to be displayed. After the presentation of the screenbuffer image has finished
the buffers swap, thus allowing the frames to be displayed smoothly one after
another. In order to render into these framebuffers we first need to acquire it
them from the presentation engine and then return it. The returning of the
buffer informs the driver that we want to display that image on screen. How To use a
swapchain in our engine we need to add a device level extension called
"VK_KHR_SWAPCHAIN_NAME" on an instance that has WSI extensions enabled (check
this).
*** Buffers
Buffers and images can be used for various purposes: in shaders, we can read or
sample data from them, or store data in them. Buffers can also store vertex
attributes, indices, or parameters used during indirect drawing. Buffers can be
used for various purposes. They can be used in pipelines via descriptor sets to
back data stores for uniform buffers, storage buffers, or texel buffers, among
others. They can be a source of data for vertex indices or attributes, or can be
used as staging resources-- intermediate resources for data transfer from the
CPU to the GPU. To use a buffer for our application we first need to allocate a
memory object and bind it to the buffer. To allocate a memory object for a
buffer we need to know what memory types are available on our physical device.
Directly mapping memory data from our application requires the memory type
host-visible. Buffers are bound to a pipeline using descriptor sets and can then
be used as a source of data in the shaders. We use a dynamic buffer for the
model matrix of the game objects.
*** Images
Buffers represent linear arrays of data. Image represent one-, two-, or
three-dimensional data organized in a way that is specified by the given
hardware. Images can also be used as color or depth/stencil attachments (render
targets), which means that we can render into them. Usually represented as image
view.
*** Uniform Buffers
Uniform buffers are used to provide values for read-only uniform variables inside shaders.
*** Storage Images
When we want to store data in imgaes from within shaders we need to use storage
images. Samplers can not be used for storage images.
*** Command Buffers
The low level control of Vulkan is also achieved through the way we communicate
and interact with the engine. In Vulkan we use command buffers to specify when
and which commands are sent to the hardware to be executed. Command buffers can
be recorded in multiple threads unlike high level API's like OpenGL where the
commands are implicitly sent to the hardware without developer influence. More
responsiblity but also more flexibility and control. This means we also need to
control not only what we submit but also when we submit it (semaphores and
fences) Commands are first recorded and then submitted to the targeted queue.
The recording allows the reuse of already used command queues. Needed to Draw,
copy memory ect. Semaphores are used to synchronize the device queues as some
commands may require a command from another queue to be finished before they are
executed themselves. Semaphores of a command buffer are associated with a
pipeline stage. Fences on the otherhand are used to check whether a command has
already finished or is still being processed. The function "vkQueueWaitIdle()"
pauses the application until all work (processing of the command buffers)
submitted to the given queue is finished. This should only be used rarely as the
GPU is much faster than the CPU and may require work to be constantly submitted
to utilize all available perfomance.
*** Command Pools
Command pools are the objects which hold the recorded commands, it is basicly a
memory pool for command buffers. They also inform the driver about the intended
usage of the command buffers that allocate their storage from them (the memory
for this is allocated dynamically and implicitly) and how they should be freed
after. Command pools also determine the queue families the command buffers can
be submitted to. From a command pool the commands are allocated and then
recorded to command buffers. Create seperate command pools for each thread of
the application that should be able to record command buffers to improve
perfomance and minimize the required synchronization.
- Primary command buffers are directly submitted to queues and can execute secondary command buffers
- Secondary command buffers can only be executed from primary command buffers
  and can not be submitted. Executing a secondary command buffer from a primary
  one does not inherit the state of the primary buffer except in a render pass.
In Vulkan the state of command buffers is not defined this means that we need to
prepare the appropriate buffers beforehand. Example: The drawing command uses
vertex attributes and indices so we need to bind the vertex data buffer and
vertex indice buffer in preparation.
- vkBeginCommandBuffer()
- vkEndCommandBuffer()
Starting another record operation implicitly resets the command buffer but this
can also be done explicitly. Resetting the command pool reset all command
buffers allocated from that pool.
*** Descriptor Set Layout
Descriptor sets gather many resources (descriptors) in one object. They are later bound to
a pipeline to establish an interface between our application and the shaders. But for the
hardware to know what resources are grouped in a set, how many resources of each type
there are, and what their order is, we need to create a descriptor set layout. The descriptor set layout specifies the internal structure of a descriptor set and, at the same
time, strictly defines what resources can be bound to the descriptor set (we can't use
resources other than those specified in the layout).
When we want to create layouts, we need to know what resources (descriptor types) will be
used and what their order will be. The order is specified through bindings--they define the
index (position) of a resource within a given set and are also used inside shaders (with a set
number through a layout qualifier) to specify a resource we want to acces.
*** Descriptor Sets
*** Sampler
Samplers consolidate parameters that determine how image data is loaded and read
inside shaders. This is called sampling. Sampled images are used to read data
from textures inside shaders. (To use an image as a sampled image it must be
created with VK_IMAGE_USAGE_SAMPLED_BIT) It may increase perfomance to combine a
sampler and a sampled image object into a combined sampler object. When we want
to use an image as a sampled image, before we load data from it inside shaders,
we need to transition the image's layout to
VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL.
*** Attachments
Attachments are images into which we render during drawing commands, inside
render passes. In other words, they are render targets.
*** Input Attachments
 Input attachments are
image resources from which we can read  data inside fragment
shaders.
*** Render Pass
In Vulkan, rendering operations are gathered into render passes. Each render pass has at
least one subpass, but can have more. If we render to an attachment in one subpass, we can
then use it as an input attachment and read data from it in subsequent subpasses of the
same render pass. It is in fact the only way to read data from attachments in a given render
pass--images serving as attachments in a given render pass can only be accessed through
input attachments inside shaders (they cannot be bound to descriptor sets for purposes
other than input attachments).
*** Framebuffer
*** Shader
*** Normal mapped geometry
*** Model Matrix
** Engine Structure
Obviously the graphics pipeline is only part of the engine. Every game engine
needs to handle the games resources such as the scenes, game objects, , . During
the development a high importance was given to make the engine work as efficient
as possible aswell as a. In the following paragraphs we will explain which
systems are already in place and how they were implemented.
*** Resource Allocation
To increase the performance of the engine we want to make sure that the loading
of resources such as a texture map or a mesh is never done redundantly, which is
likely the case in a puzzle game as key components are similar between different
scenes. In order to implement this we allocate buffers upfront to store all our
resources and a hashmap that maps the file paths of the loaded resources to
their pointers in memory. If a resource becomes necessary in a scene, we can
cross check whether the file path has already been loaded and then reuse the
already loaded file instead of reloading it. This means that we will only load
the difference between two levels which will reduce load times and create a smoother
gameplay experience for the player. The Hashmaps also provide further advantage
for the memory management as we can free the memory and GPU memory for the
texture resources by iterating over the hashmap and can incorporate this in the
scene load/unloading process.

*** Bucket Allocator
Meshes, textures, scenes all need to live in memory somewhere. But instead of
heap allocating them all separately, we wrote an allocator to keep them
together. The bucket allocator is basically a dynamic array of buckets, which
are fixed-sized arrays. On startup the bucket allocator allocates itself a chunk
of memory to hold the initial amout of buckets. When later all buckets are full,
it allocates more buckets. No entries need to be copied, the only thing that
needs to be updated is the dynamic array that holds the pointers to the buckets.
Since no elements will ever move, it is safe to store and use pointers to them
everywhere. When elements are freed, they are added to a free list, where they
will be reused on the next allocation. The bucket allocator also provides
functionality to iterate over all allocated elements. Bucket allocators are used
for:

 - Textures
 - Meshes
 - Scenes
 - Materials
 - Scheduler

** Scheduler
The scheduler manages active animations and scheduled actions.

Animations are given by a start time, an end time, an aribitrary interpolant and
an interpolation type. We can animate any variable in memory. One example for
this animated field of view of the camera when finishing a level. It would also
be possible to animate single vertices or material parameters but this is not in
use at the moment.

The currently supported interpolant types are:
 - vectors
 - quarternions
 - floating point numbers

More can be added later if the need arises. The basic interpolation functions
where the ease functions just manipulate the variable ~t~ $\in [0; 1]$ are:
| type                                  | adjustment for t                            |
|---------------------------------------+---------------------------------------------|
| linear interpolation (also spherical) |                                             |
| quadratic ease-in                     | ~t = t*t;~                                  |
| quadratic ease-out                    | ~t = -(t*(t-2));~                           |
| quadratic ease-in and ease-out        | ~t = (t<0.5) ? (2*t*t) : (-2*(t*(t-2))-1);~ |


With this functionality, you can schedule even chains of animations in advance
and continue with your game loop, as the scheduler will update the interpolants
for all active animations every frame.

As an example, if you would want to animate a jump, where the horizontal
movement is linear, while the vertical is quadratic you could split up the
animations in three parts which are scheduled together:
- The upward movement, which is interpolated with ease-out
- The downward movement, which is interpolated with ease-in
- The horizontal movement, which is interpolated with linear interpolation

#+begin_src c++
f32 from_z = qubi.transform.position.z;
f32 to_z   = qubi.transform.position.z + 1;
f32 from_x = qubi.transform.position.x;
f32 to_x   = qubi.transform.position.x + 2;

Scheduler::schedule_animation({ // upward movement
    .seconds_to_start   = 0,
    .seconds_to_end     = 0.6,
    .interpolant        = &qubi.transform.position.z,
    .interpolant_type   = Interpolant_Type::F32,
    .from               = &from_z,
    .to                 = &to_z,
    .interpolation_type = Interpolation_Type::Ease_Out,
});
Scheduler::schedule_animation({ // downward movement
    .seconds_to_start   = 0.6,
    .seconds_to_end     = 1.2,
    .interpolant        = &qubi.transform.position.z,
    .interpolant_type   = Interpolant_Type::F32,
    .from               = &to_z,
    .to                 = &from_z,
    .interpolation_type = Interpolation_Type::Ease_In,
});
Scheduler::schedule_animation({ // horizontal movement
    .seconds_to_start   = 0,
    .seconds_to_end     = 1.2,
    .interpolant        = &qubi.transform.position.x,
    .interpolant_type   = Interpolant_Type::F32,
    .from               = &from_x,
    .to                 = &to_x,
    .interpolation_type = Interpolation_Type::Lerp,
});
#+end_src

With this capability, it is easy to procedurally generate the animations that we
need for our game. Of course in our case, the cube does not jump, but for more
complex scenarios, like when flipping from dry tiles onto ice, start sliding and
flip back on a dry tile, it is possible now to deterministically compute the
resulting game state after every key input, and schedule the animations that
lead to it.

If course, during the animations -- so while the cube is sliding or flipping --
player inputs should not impact it's trajectory. For that you can give the
scheduler a "lock" which is just a pointer to a boolean for now, which will be
set to =true= as soon as the animation is scheduled, and which will be set to
=false= as soon as the animation finished. For now this is good enough as we
expect to run the animation code on the same thread as the user input code. So
with this we have a =animation_locked= boolean variable whaich we can check on
user input, to check if we actually want to compute a player movement, or just
keep the button in the player's input buffer, so it will be used as soon as
=animation_locked= becomes false again.

Another thing that need to happen, is to check if the player finished the level
as soon as the movement finishes. To do this, we don't check every frame for the
finish condition, but rather schedule an action that checks for the finish
condition on the exact time the animation finishes. Actions basically consist of
a timer when they should run, and a functionpointer that will be called at that
time; and since captureless lambdas kann be cast to function pointers we can
even write them inline.

#+begin_src c++
Scheduler::schedule_action({
    .seconds_to_run = animation_end_time,
    .lambda = [](){
      // check for finish condition
    }
});
#+end_src

C++ closures cannot be used as an action, as their size in memory varies, and
thus cannot neatly be arranged in the bucket allocator holding all the actions
(unless you use more levels of abstraction, like with =std::function= which
themself heap allocate memory). On occasions we would need variable capture,
actions have a fixed amount of space that can be used to store parameters to the
function that should be scheduled.

Internally the Scheduler just consists of two bucket allocators, one for
animations and one for actions. The scheduler gets called once per frame to
update the animations and call the actions that are due.

The timestamps are stored as performance counters, since the easiest way to get
a high resolution clock seems to be by calling =QueryPerformanceCounter= on
Windows, and we wrote a similar function for linux.

** Movement
Having a deterministic animation system is important for the player's movement,
as our game is a puzzle game, where movements have to be exact. In our case, the
game world consists of 2 tile types the player can be on: slippery and dry tiles.

We calculate the future gamestate for every input the user gives. This can be an
iterative process, since one movement forces the cube into another one. This
happens for example when standing on a dry tile and moving onto a slippery tile:
The cube will flip onto the slippery ground and then immidiately start sliding
in the same direction. So while simulating the future game state iteratively, we
also at the same time generate and schedule the animations which manifest the
movements to reach the calculated game state. This only works because we can
calculate the start and end time of each movement and schedule the animations
precisely to these times.

** Game Logic
For the Game Objects that make up our scene we have right now implemented the
following categories 'start pos', 'finish' and 'obstacles' and 'slippery tiles'.
All tiles have a specific corresponding movement (sequence of animations)
connected to them.
*** Slippery tiles
Slippery tiles are the fundamental part of the game. When the cube reaches a
slippery tile it will slide until it reaches an obstacle. The sliding animation
is computed using the Lerp function on the position values of the transform
matrix.
*** Dry tiles
When moving on or onto a dry tile the cube flips over the bottom edge that
corresponds to the direction that was input by the user. The flip movement is
made up of 3 distinct movements:
*** Finish tiles
When coming to a hold on a finish tile the camera will zoom out. Right after
that, the next level is loaded. We further gave finish tiles different
additional types. So a finish tile can either be slippery or dry. In levels in
which the cube can flip, the finish tile also has a distinction depending on
which of the three sides of the cube needs to be on top/bottom.
*** Obstacles
Hitting an obstacle leads to the cube stopping right in front of it. No further
animation was necessary here.
*** Level Loading
A early Layer 3 goal for our project was the ability to load levels from a text
file so we can streamline the level creation process that will be a major part
for the alpha release milestone. The object and structure coordinates in the
text file are grouped into categories and designated with 'begin category' and
'end category' which the map loader will then use to create a scene objects.
Additionally the finish tiles have a extra keyword that determines whether they
are slippery or dry and which color condition of the cube has to be fulfilled to
finish the level. The rest of the tiles are automatically set to slippery.
# * Game Demo
* Challenges & Design Revisions
When implementing the gameplay mechanics we encountered mostly minor issues
which were resolved rather quickly. The win condition as well as the different
behaviors of the cube when reaching specific tiles in itself were not our
biggest challenges either. Our main concerns were all in regards to the
implementation of the engine to make sure it runs smoothly.

- We spent a lot of time understanding theway to bind resources to the graphics
  pipeline and as a whole all the concepts and entities involved when rendering
  a scene on the gpu.
- Additionally we realized quickly that something like quaternions are really
  necessary if we do not want to keep track of the flips that occured, rotations
  are obviously not commutative.

However some concerns about our own goals arose:

- When we implemented input buffering, we noticed that once an animation is
  started, on the next frame the key will most likely still be pressed, and thus
  the key press would land in the input buffer, to be processed once the
  animation is finished. This is of course not as we envisioned. We made it
  necessary for the button to be be reset before it is eligible for the input
  buffer.
- And even though we implemented a method for loading levels from text files, we
  think for our purposes now, implemening them in C++ might actually have more
  benefits, as common aspects of the scene can easily be made default arguments
  or stuct members. With C++, you have the expressive power of a programming
  language to setup the scene, so that the C++ implementation is both more
  simple and concise. An advantage of loading levels at runtime however, is that
  you can edit the level while the game is running and just reload the level to
  instantly see the changes you made to the scene.

* Meta Info                                                        :noexport:
#+startup: overview
#+options: html-postamble:nil toc:nil title:nil
#+OPTIONS: ^:{}
#+macro: insert_game_name_here qubi
#+macro: insert_team_name_here FÃ¼nfKopf

#+author: Felix Brendel, Jonas Helms, Van Minh Pham
#+title: Interim Demo: {{{insert_game_name_here}}}

#+latex_header: \input{latex.tex}

